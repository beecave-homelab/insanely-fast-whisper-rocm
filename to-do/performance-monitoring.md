# To-Do: Establish Performance Monitoring and Benchmarking

This plan outlines the steps to establish a baseline for performance and monitor improvements in the `insanely_fast_whisper_api` package. We will use the CLI and a standardized test file to create reproducible benchmarks.

## Tasks

- [x] **Analysis Phase: Define Benchmark Metrics** ✅ _Completed 2025-07-06_
  - [x] Analyze the JSON output generated by the CLI to identify key performance metrics.
    - Path: `insanely_fast_whisper_api/cli/cli.py` (and sample output in `transcripts/`)
    - Action: Reviewed the structure of the output JSON and confirmed available keys.
    - Analysis Results:
      - `runtime_seconds`: Wall-clock processing time for the entire transcription.
      - `config_used`: Dict with model, device, dtype, batch_size, language, etc.
      - `processing_time_seconds`: Alias emitted in CLI console; same value as `runtime_seconds`.
      - _To be captured by benchmark script (not in JSON)_:
        - Model load time (model init → ready).
        - GPU metrics: VRAM usage, power draw, temperature (`pyamdgpuinfo`).
        - System RAM usage (`psutil`).
    - Accept Criteria: The above list defines the initial benchmark metrics.

- [x] **Implementation Phase: Create Benchmark Module** ✅ _Completed 2025-07-06_
  - [x] Develop a benchmark **utility module** that can be invoked from the CLI.
    - Path: `insanely_fast_whisper_api/utils/benchmark.py` (new file)
    - Action:
      1. Implement a `BenchmarkCollector` class that:
         - Measures total runtime (`time.perf_counter()` around core transcription call).
         - Optionally measures model-load time by instrumenting the model factory.
         - Collects GPU stats (VRAM, power, temperature) with `pyamdgpuinfo` when an AMD GPU is detected; otherwise skip gracefully.
         - Collects system RAM stats via `psutil`.
         - Writes all metrics to `benchmarks/<timestamp>_<model>.json` (and/or CSV append).
      2. Provide helper functions (e.g., `collect_metrics(config: dict, result: dict)`) that the CLI can call directly—**no subprocess invocation**.
      3. Ensure the module can be imported without heavy deps if user does not supply `--benchmark`.
      4. Create a `BenchmarkResult` Pydantic model for typing and easy JSON serialization.
      5. Update development dependencies (`psutil`, `pyamdgpuinfo`) in `pyproject.toml` and README.
  - [x] Refactor CLI to inject benchmarking:
    - Path: `insanely_fast_whisper_api/cli/commands.py`
    - Action: Add a global `--benchmark` flag. When the flag is present:
      1. Instantiate `BenchmarkCollector` before the transcription call.
      2. After transcription, pass `config_used` and `runtime_seconds` (already available) to the collector.
      3. Collector saves metrics JSON/CSV to `benchmarks/`.
    - Accept Criteria: Normal CLI output remains unchanged; extra benchmark file created when flag supplied.
  - [x] Create `benchmarks/` directory in repo (git-ignored) if it does not exist. ✅ _Auto-created by BenchmarkCollector_
  - [ ] Ensure API remains scriptable so external tools can still call the CLI functions programmatically. (_Pending_)
    - Status: `Pending`

- [x] **Implementation Phase: Add --benchmark Flag** ✅ _Completed 2025-07-06_
  - [x] Add a `--benchmark` flag to the main CLI commands (`insanely_fast_whisper_api/cli/commands.py`). ✅ _Implemented_
    - Path: `insanely_fast_whisper_api/cli/commands.py`
    - Action: Implement the `--benchmark` flag to record the defined metrics and write them to the `benchmarks/` directory without altering normal transcription output.
    - Accept Criteria: The `--benchmark` flag is implemented and functional.

- [x] **Testing Phase: Validate Benchmark Integration**
  - [x] Ensure the benchmark flag works correctly and produces accurate results.
    - Path: `insanely_fast_whisper_api/cli/commands.py` & `insanely_fast_whisper_api/utils/benchmark.py`
    - Action: Run the CLI on `tests/conversion-test-file.mp3` with and without `--benchmark`. Verify:
       1. Transcription output unchanged.
       2. Benchmark JSON file created in `benchmarks/` with expected keys.
       3. GPU/RAM metrics recorded when available, otherwise skipped gracefully.
       4. Error handling works (e.g., invalid path for benchmarks directory).

       _Results_: All criteria met. Fixed filename sanitization and Pydantic v2 serialization to resolve previous `dumps_kwargs` error.
  - [x] Accept Criteria: CLI executes without errors and benchmark file contents are valid.

### Benchmark Detail Improvements (Phase 3)

- [x] **Filename Generation Consistency** – Use `insanely_fast_whisper_api/utils/filename_generator.py` to generate benchmark filenames; prepend `benchmark_` to the standard pattern.
  - Path: `insanely_fast_whisper_api/utils/benchmark.py`, `insanely_fast_whisper_api/utils/filename_generator.py`
  - Accept Criteria: Saved JSON filename follows the same convention as transcripts, preceded by `benchmark_`, and contains no path separators.
- [x] **GPU Statistics Collection** – Capture VRAM usage, power, temperature for AMD cards (and CUDA if available).
  - Path: `insanely_fast_whisper_api/utils/benchmark.py`
  - Accept Criteria: `gpu` section in JSON populated when hardware metrics are available; gracefully fallback to `null` otherwise.
- [x] **Torch Version in System Metrics** – Include PyTorch version under `system.torch_version`.
  - Path: `insanely_fast_whisper_api/utils/benchmark.py`
  - Accept Criteria: JSON shows correct Torch version string.
- [x] **Extra Stats Support** – Allow CLI to pass an `extra` dict (e.g., model precision, tokenizer latency) to BenchmarkCollector and persist it.
  - Path: `insanely_fast_whisper_api/cli/commands.py`, `insanely_fast_whisper_api/utils/benchmark.py`
  - Accept Criteria: `extra` field in JSON contains supplied data when provided.

---

- [x] **Phase 3: Conditional Transcript Export**
  - [x] Modify CLI so that when `--benchmark` is used and the user did *not* explicitly supply `--export-format`, transcript files in `transcripts/` are **not** written; only the benchmark JSON is saved.
  - Accept Criteria: Running `cli … --benchmark` alone writes only the JSON in `benchmarks/`; running with `--benchmark --export-format json` (or other) behaves like current export.

- [x] **Documentation Phase: Document Benchmarking Process**
  - [x] Update `project-overview.md` with a new section on performance monitoring.
    - Path: `project-overview.md`
    - Action: Add a "Performance Benchmarking" section that explains the purpose of the benchmark script, how to run it, and how to interpret the results. Document any additional dependencies introduced by the benchmarking tool (e.g., `psutil`, `pandas`) and update installation instructions as needed.
    - Accept Criteria: The documentation clearly explains the benchmarking process to other developers. ✅

## Related Files

- `scripts/benchmark.py` (new)
- `insanely_fast_whisper_api/cli/cli.py` (referenced)
- `tests/conversion-test-file.mp3` (referenced)
- `project-overview.md` (to be updated)
- `to-do/performance-monitoring.md` (this file)

## Future Enhancements

- [ ] Automate the benchmark script to run in a CI/CD pipeline on every major change.
- [ ] Extend the script to capture and report on VRAM/RAM usage during transcription.
- [ ] Add functionality to generate plots or visualizations for performance comparisons over time.
